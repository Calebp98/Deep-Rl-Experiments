{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Overview\n",
    "\n",
    "### Introduction\n",
    "- Assume a neural network composed of three layers of neurons: input layer, hidden layer and output layer. We wish to train this network so when it is presented with a specific input it will have a certain output.\n",
    "- For this lab we are going to work with the [MNIST database of handwritten digits](http://yann.lecun.com/exdb/mnist) and train our network to recognise handwritten 0-9 digits. The MNIST database contains 60000 images which can used to train the network and 10000 more images for testing purposes. Each image is an  matrix representing a digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn.png\" alt=\"neural-net\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "- **Network initialisation:**\n",
    "\n",
    "\n",
    "1. Generate a set of weights between the input and the hidden layer. The input layer should have 784 neurons, one for each pixel of the image.\n",
    "\n",
    "2. Generate a set of weights between the hidden and the output layer. The output layer should have 10 neurons, one for each digit.\n",
    "\n",
    "3. Generate two sets of bias, one for the hidden layer and one for the output layer. Bias is set to 1 and it is an extra input to the neurons in order to avoid zero activation (all inputs equal to 0).   \n",
    "\n",
    "\n",
    "- **Feedforward:**\n",
    "\n",
    "\n",
    "1. Feed an image $\\vec{x_k}$ to the network,\n",
    "   \n",
    "\n",
    "2. Compute the input to each of the neurons of the hidden layer, $act_{h_i} = \\sum_{i=1}^{784}w_i x_i + bias$ and their outputs with the use of the [sigmoid function](http://mathworld.wolfram.com/SigmoidFunction.html),$out_{h_i} = \\frac{1}{(1+e^{-act_i})}$.\n",
    "\n",
    "3. Repeat for the output layer.\n",
    "\n",
    "<img src=\"nn1.png\" alt=\"neural-net\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "- **Error:**\n",
    "\n",
    "1. Calculate the error in the output of the neural network. Given as input a zero image it is expected that the first neuron of the input layer to be fully active, i.e has an output of 1, while the rest neuron to have outputs of 0. Thus the target output would be $target = [1,0,0,\\dots,0]^T$ and given that the output of the neural network is some $output$ the error is given by the formula $E = \\sum_{i=1}^{n}\\frac{1}{2}(target_i-output_i)^2$ where $n$ is the number of neurons in the output layer of the network. \n",
    "\n",
    "<img src=\"nn2.png\" alt=\"neural-net\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "- **Backpropagation:**\n",
    "\n",
    "\n",
    "1. Apply the delta rule between the output and the hidden layers. For the weight $w_{ij}$ we have $\\frac{\\vartheta E}{\\vartheta w_{\\textrm{ij}} }=\\frac{\\vartheta E}{\\vartheta {\\textrm{out}}_{o_i } }\\;\\frac{\\vartheta {\\textrm{out}}_{o_i } }{\\vartheta {\\textrm{act}}_{o_j } }\\;\\frac{\\vartheta {\\textrm{act}}_{o_j } }{\\vartheta w_{\\textrm{ij}} }\\;=-\\left({\\textrm{target}}_i -{\\textrm{output}}_i \\right){\\;\\textrm{out}}_{o_i } \\;\\left(1-{\\textrm{out}}_{o_i } \\right)\\;{\\textrm{out}}_{h_j }$, where $\\delta_{o_i } =\\left({\\textrm{target}}_i -{\\textrm{output}}_i \\right){\\;\\textrm{out}}_{o_i } \\;\\left(1-{\\textrm{out}}_{o_i } \\right)$.\n",
    "\n",
    "2. Update the weights between the hidden and the output layers.  For the weight $w_{ij}$ we have ${\\Delta w}_{\\textrm{ij}} =-\\eta \\delta_{{\\textrm{out}}_i } {\\textrm{out}}_{h_j }$.\n",
    "\n",
    "3. Repeat steps 1 and 2 between the hidden and the input layes and update the weights between them. Here for the weight $w_{ij}$ we have $\\frac{\\vartheta E}{\\vartheta w_{\\textrm{ij}} }=\\frac{\\vartheta E}{\\vartheta {\\textrm{out}}_{h_i } }\\;\\frac{\\vartheta {\\textrm{out}}_{h_i } }{\\vartheta {\\textrm{act}}_{h_j } }\\;\\frac{\\vartheta {\\textrm{act}}_{h_j } }{\\vartheta w_{\\textrm{ij}} }={\\textrm{out}}_{h_i } \\left(1-{\\textrm{out}}_{o_i } \\right)\\left(w_{\\textrm{ij}}^{\\left(2\\right)} \\delta_{o_j }^{\\left(2\\right)} \\right)$, where the terms $w_{\\textrm{ij}}^{\\left(2\\right)} \\delta_j^{\\left(2\\right)}$ are the weights and the delta function of the previous layer.\n",
    "\n",
    "4. Update the weights between the hidden and the input layers. For the weight $w_{ij}$ we have ${\\Delta w}_{\\textrm{ij}} =-\\eta \\delta_{h_i } x_j$.\n",
    "\n",
    "<img src=\"nn3.png\" alt=\"neural-net\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The code provided first loads the MNIST database and generates a neural network using the parameters provided. Then it trains the network and tests it using the testing set of the MNIST database.\n",
    "\n",
    "1. Tune the network by finding appropriate values for the parameters: number of epochs, number of neurons of the hidden layer and learning rate.\n",
    "\n",
    "2. Test the network using the test dataset and compute it's accuracy.\n",
    "\n",
    "3. The weights are generating from a uniformly distribution in the interval (0,1) and are then normalized using the sum equals to 1 normalization, i.e the summation of the inputs should be equal to 1. Why is normalization important? Another initialisation method is the [Xavier initialization](http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf?hc_location=ufi), which is more commonly used in deep neural networks of more than one hidden layer\n",
    "\n",
    "4. Upgrade the network by adding one more hidden layer, derive the appropriate formulas for the feedforward and the backpropagation and program them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import numpy.matlib \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train set\n",
    "x_train = np.loadtxt('train-images.idx3-ubyte.txt')\n",
    "# Read the train labels\n",
    "trainlabels = np.loadtxt('train-labels.idx1-ubyte.txt')\n",
    "# Read the test set\n",
    "x_test = np.loadtxt('t10k-images.idx3-ubyte.txt')\n",
    "# Read the test labels\n",
    "testlabels = np.loadtxt('t10k-labels.idx1-ubyte.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.zeros((10,trainlabels.shape[0]))\n",
    "y_test=np.zeros((10,testlabels.shape[0]))\n",
    "\n",
    "for i in range(0,trainlabels.shape[0]):\n",
    "    \n",
    "    y_train[trainlabels[i].astype(int),i]=1\n",
    "    \n",
    "for i in range(0,testlabels.shape[0]):\n",
    "    \n",
    "    y_test[testlabels[i].astype(int),i]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p,n]=x_train.shape\n",
    "nlabels=10\n",
    "\n",
    "[p2,n2]=x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch=45\n",
    "n_batch=32\n",
    "n_input_layer=p #number of neurons on input layer\n",
    "n_hidden_layer=10 #number of neurons on hidden layer\n",
    "n_output_layer=nlabels #number of neurons on output layer\n",
    "eta=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a simple network\n",
    "#For W1 and W2, columns are the input and the rows are the output.\n",
    "#W1: Number of columns (input) needs to be equal to the number of features \n",
    "#    of the  MNIST digits, thus p. Number of rows (output) should be equal \n",
    "#    to the number of neurons of the hidden layer thus n_hidden_layer.\n",
    "#W2: Number of columns (input) needs to be equal to the number of neurons \n",
    "#    of the hidden layer. Number of rows (output) should be equal to the \n",
    "#    number of digits we wish to find (classification).\n",
    "\n",
    "# create random matrix of weight (uniformally distrubuted)\n",
    "W1=np.random.uniform(0,1,(n_hidden_layer,n_input_layer)); \n",
    "# normalise w1 (not really sure why but i can live with this)\n",
    "W1=np.divide(W1,np.matlib.repmat(np.sum(W1,1)[:,None],1,n_input_layer));\n",
    "\n",
    "W2=np.random.uniform(0,1,(n_output_layer,n_hidden_layer));\n",
    "W2=np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the biases\n",
    "\n",
    "# set all biases to one\n",
    "bias_W1=np.ones((n_hidden_layer,))\n",
    "bias_W2=np.ones((n_output_layer,))\n",
    "\n",
    "# Keep track of the network inputs and average error per epoch\n",
    "errors=np.zeros((n_epoch,));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "\n",
    "# do this for every epoch\n",
    "for i in range(0,n_epoch):\n",
    "\n",
    "# for every element of the batch\n",
    "    for j in range(0,n_batch):\n",
    "        \n",
    "        # Input (random element from the dataset)\n",
    "        idx=np.random.randint(0,n); # get some index\n",
    "        x=x_train[:,idx] #set x to a certain image (in vector form)\n",
    "        \n",
    "        # Neural activation: input layer -> hidden layer\n",
    "        act1=np.dot(W1,x)+bias_W1 # calculate activations of next layer (just dot product of image and weights + bias)\n",
    "        # Apply the sigmoid function\n",
    "        out1=1/(1+np.exp(-act1)) #sigmoidal function from maths and stuff\n",
    "    \n",
    "        # Neural activation: hidden layer -> output layer\n",
    "        act2=np.dot(W2,out1)+bias_W2 # then do the same thing for the next layer\n",
    "        # Apply the sigmoid function\n",
    "        out2=1/(1+np.exp(-act2))\n",
    "        \n",
    "#         # Neural activation: hidden layer -> output layer\n",
    "#         act3=np.dot(W2,out2)+bias_W3 # then do the same thing for the next layer\n",
    "#         # Apply the sigmoid function\n",
    "#         out3=1/(1+np.exp(-act3))\n",
    "        \n",
    "        # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "        desired_output=y_train[:,idx] #get label\n",
    "\n",
    "#         #  extra layer for task      \n",
    "#         # Backpropagation: output layer -> hidden layer\n",
    "#         out3delta=out3*(1-out3)*(out3-desired_output) \n",
    "#         W3=W3-eta*np.outer(out3delta,out2)\n",
    "\n",
    "        # Backpropagation: hidden layer -> hidden layer\n",
    "        out2delta=out2*(1-out2)*(out2-desired_output) \n",
    "        W2=W2-eta*np.outer(out2delta,out1)\n",
    "        \n",
    "        # Backpropagation: hidden layer -> input layer\n",
    "        out1delta=out1*(1-out1)*np.dot(W2.T,out2delta)\n",
    "        W1=W1-eta*np.outer(out1delta,x)\n",
    "        \n",
    "        # Store the error per epoch\n",
    "        errors[i]=errors[i]+np.sum((out2-desired_output)*(out2-desired_output))/n_batch\n",
    "        \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the performance\n",
    "plt.plot(errors)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Average error per epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the test set to compute the network's accuracy\n",
    "\n",
    "for j in range(0,n_batch):\n",
    "\n",
    "    # Input (random element from the dataset)\n",
    "    idx=np.random.randint(0,n2); # get some index\n",
    "    x=x_test[:,idx] #set x to a certain image (in vector form)\n",
    "\n",
    "    # Neural activation: input layer -> hidden layer\n",
    "    act1=np.dot(W1,x)+bias_W1 # calculate activations of next layer (just dot product of image and weights + bias)\n",
    "    # Apply the sigmoid function\n",
    "    out1=1/(1+np.exp(-act1)) #sigmoidal function from maths and stuff\n",
    "\n",
    "    # Neural activation: hidden layer -> output layer\n",
    "    act2=np.dot(W2,out1)+bias_W2 # then do the same thing for the next layer\n",
    "    # Apply the sigmoid function\n",
    "    out2=1/(1+np.exp(-act2))\n",
    "    \n",
    "    # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "    desired_output=y_test[:,idx] #get label\n",
    "    \n",
    "# Store the error on the test set\n",
    "errors=np.sum((out2-desired_output)*(out2-desired_output))/n_batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03551150037984231"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
